---
title: "NeuralEstimators: Incomplete gridded data"
author: "Matthew Sainsbury-Dale, Andrew Zammit-Mangion, and RaphaÃ«l Huser"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{NeuralEstimators: Incomplete gridded data}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```


```{r}
library("NeuralEstimators")
library("JuliaConnectoR")
library("ggplot2")
library("parallel") # mclapply
library("dplyr")
library("egg")
juliaEval('using NeuralEstimators, Flux')
juliaEval('using BSON: @load')

savepath <- file.path("intermediates", "architecture")
dir.create(savepath, recursive = TRUE, showWarnings = FALSE)
```

# Data simulation 

Here, we consider the spatial Gaussian process model, where $\boldsymbol{Z} \equiv (Z_{1}, \dots, Z_{n})'$ are data collected at locations $\{\boldsymbol{s}_{1}, \dots, \boldsymbol{s}_{n}\}$ in a spatial domain that is a subset of $\mathbb{R}^2$. The data are assumed to be spatially-correlated mean-zero Gaussian random variables with exponential covariance function, 
$$
\textrm{cov}(Z_i, Z_j) = \textrm{exp}(-\|\boldsymbol{s}_i - \boldsymbol{s}_j\|/\theta),
$$
with unknown range parameter $\theta > 0$. Here, we take the spatial domain to be the unit square, we simulate data on a grid with $16^2 = 256$ possible observation locations, and we adopt a uniform prior, $\theta \sim \rm{Unif}(0, 0.5)$. 

To begin, we define a function for sampling from the prior distribution and a function for marginal simulation from the statistical model:

```{r}
# Sampling from the prior distribution
# K: number of samples to draw from the prior
prior <- function(K) { 
  theta <- 0.5 * runif(K) # draw from the prior distribution
  theta <- t(theta)       # reshape to matrix
  return(theta)
}

# Marginal simulation from the statistical model
# theta: a matrix of parameters drawn from the prior
# m: number of conditionally independent replicates for each parameter vector
# N: number of locations along each dimension of the grid
simulate <- function(theta, m = 1, N = 16) { 
  
  if (length(N) == 1) N <- c(N, N)
  
  # Spatial locations, a grid over the unit square, and spatial distance matrix
  # S <- expand.grid(seq(1, 0, len = N[1]), seq(0, 1, len = N[2]))
  
  # Define the original N to establish the reference spacing
  N_ref <- c(16, 16)
  
  # Calculate the grid spacing based on the reference grid
  spacing_x <- 1 / (N_ref[1] - 1)  # Spacing in the x-direction
  spacing_y <- 1 / (N_ref[2] - 1)  # Spacing in the y-direction
  
  # Calculate the new range for the grid to preserve the spacing
  range_x <- (N[1] - 1) * spacing_x
  range_y <- (N[2] - 1) * spacing_y
  
  # Construct the grid with the same spacing as the reference grid
  S <- expand.grid(seq(0, range_x, length.out = N[1]), 
                   seq(0, range_y, length.out = N[2]))
  
  
  # Spatial distance matrix
  D <- as.matrix(dist(S))         
  
  # Simulate conditionally independent replicates for each parameter vector
  Z <- mclapply(1:ncol(theta), function(k) {
    Sigma <- exp(-D/theta[k])  # covariance matrix
    L <- t(chol(Sigma))        # lower Cholesky factor of Sigma
    n <- nrow(L)               # number of observation locations
    mm <- if (length(m) == 1) m else sample(m, 1) # allow for variable sample sizes
    z <- matrix(rnorm(n*mm), nrow = n, ncol = mm) # standard normal variates
    Z <- L %*% z               # conditionally independent replicates from the model
    Z <- array(Z, dim = c(N[1], N[2], 1, mm)) # reshape to multidimensional array
    Z
  }, mc.cores = detectCores() - 1) 
  
  return(Z)
}
```

Construct training and validation sets:

```{r}
K <- 50000 # size of the training set 
theta_train <- prior(K)          # parameter vectors used in stochastic-gradient descent during training
theta_val   <- prior(K/10)       # parameter vectors used to monitor performance during training
Z_train <- simulate(theta_train) # data used in stochastic-gradient descent during training
Z_val   <- simulate(theta_val)   # data used to monitor performance during training
```

# Architectures 

Now, define our neural-network architectures. The first two are based on the architecture used in our ARSIA paper. The third was proposed by Gerber and Nychka, and we used it in our TAS/JMLR papers, and it is the architecture that we have used so far in our neural EM project. The fourth is a deeper, 'ResNet' style architecture. 

```{r}
architecture1 <- juliaEval('
  function architecture1()
    psi = Chain(
      Conv((3, 3), 1 => 32, relu),   
      MaxPool((2, 2)),               
      Conv((3, 3), 32 => 64, relu),  
      MaxPool((2, 2)),               
      Flux.flatten                   
    )
    phi = Chain(
          Dense(256, 512, relu),        
          Dense(512, 1)
      )
    PointEstimator(DeepSet(psi, phi))  
  end
')

architecture2 <- juliaEval('
  function architecture2()
    psi = Chain(
      Conv((3, 3), 1 => 64, relu),   
      MaxPool((2, 2)),               
      Conv((3, 3), 64 => 128, relu),  
      MaxPool((2, 2)),               
      Flux.flatten, 
      Dense(512, 256) 
    )
    phi = Chain(
          Dense(256, 512, relu),        
          Dense(512, 1)
      )
    PointEstimator(DeepSet(psi, phi))  
  end
')

architecture3 <- juliaEval('
  function architecture3()
    psi = Chain(
    	Conv((10, 10), 1 => 64, relu),
    	Conv((5, 5),  64 => 128,  relu),
    	Conv((3, 3),  128 => 256, relu),
    	Flux.flatten                
    )
    phi = Chain(
          Dense(256, 512, relu),        
          Dense(512, 1)
      )
    PointEstimator(DeepSet(psi, phi))  
  end
')

architecture4 <- juliaEval('
  function architecture4()
    psi = Chain(
      Conv((3, 3), 1=>16, pad=1, bias=false), BatchNorm(16, relu),   # Initial Conv layer
      ResidualBlock((3, 3), 16 => 16),                               # Residual Block 1
      ResidualBlock((3, 3), 16 => 32, stride=2),                     # Residual Block 2
      ResidualBlock((3, 3), 32 => 64, stride=2),                     # Residual Block 3
      ResidualBlock((3, 3), 64 => 128, stride=2),                    # Residual Block 4
      GlobalMeanPool(),                                              # Global pooling
      Flux.flatten,
      Dense(128, 128)                                                # Fully connected layer
    )
    phi = Chain(
          Dense(128, 512, relu),        
          Dense(512, 1)
      )
    PointEstimator(DeepSet(psi, phi))  
  end
')

architectures <- list(architecture1, architecture2, architecture3, architecture4)

# Number of parameters in each architecture
sapply(architectures, function(arch) juliaLet('nparams(arch())', arch = arch))
# 150913 337921 638657 390321
```




# Constructing an ensemble of estimators

For each architecture, train $J$ estimators independently: 

```{r}
J <- 10 # number of ensemble components

all_estimators <- lapply(seq_along(architectures), function(i) {
  cat("Training estimators with architecture", i, "\n")
  lapply(1:J, function(j) {
      cat("Training estimator", j, "\n")
      train(
        architectures[[i]](),
        theta_train = theta_train,
        theta_val = theta_val,
        Z_train = Z_train,
        Z_val = Z_val, 
        savepath = file.path(savepath, paste0("architecture", i), paste0("estimator", j))
      )
  }) 
})
```

```{r}
## Average training time for each estimator 
sapply(seq_along(architectures), function(i) {
  train_times <- sapply(1:J, function(j) {
      file_path = file.path(savepath, paste0("architecture", i), paste0("estimator", j), "train_time.csv")
      read.csv(file_path, header = FALSE)[1, 1]
  }) 
  mean(train_times)
})
# 323.7637  439.1885  519.0239 1953.1654
```

```{r}
loadbestmodel <- function(estimator, path) {
  juliaLet(
    '
    using NeuralEstimators, Flux 
    using BSON: @load
    model_state = Flux.state(estimator)
    model_path = joinpath(path, "best_network.bson")
    @load  model_path model_state
    Flux.loadmodel!(estimator, model_state)
    estimator
    ',
    estimator = estimator, path = path
    )
}

# load the estimators (allows starting from here without retraining)
all_estimators <- lapply(seq_along(architectures), function(i) {
  lapply(1:J, function(j) {
    estimator <- architectures[[i]]()
    loadbestmodel(estimator, file.path(savepath, paste0("architecture", i), paste0("estimator", j)))
  })
})
```

```{r}
Ensemble <- function(estimators) juliaLet('Ensemble(estimators)', estimators = estimators)
all_ensembles <- lapply(all_estimators, Ensemble)
```


# Results

Generate a test set for assessing the performance of the estimators: 

```{r}
set.seed(1)
theta_test <- prior(2500)
Z_test <- simulate(theta_test)
```

```{r}
# log-likelihood 
# allows for (conditionally) independent replicates stored in the fourth 
# dimension of Z, but these replicates must have the same missingness pattern.
log_lik <- function(theta, Z) {
  
  # Spatial locations and distance matrix
  N <- nrow(Z) # number of locations along each dimension of the grid
  S <- expand.grid(seq(1, 0, len = N), seq(0, 1, len = N))
  D <- as.matrix(dist(S))  
  I <- which(!is.na(c(Z[, , 1, 1]))) # indices of observed elements (assumed constant between replicates)
  D <- D[I, I]
  
  # Covariance matrix and its Cholesky factor
  Sigma <- exp(-D / theta)
  L <- t(chol(Sigma))
  
  # Compute the log determinant term: 2 * sum(log(diag(L)))
  log_det_term <- 2 * sum(log(diag(L)))
  
  # log-likelihood for each replicate
  ll <- apply(Z, MARGIN = 4, FUN = function(z) {
      
      # Observed elements, converted to vector
      z <- c(z)[I] 
      n <- length(z)
      
      # Compute L^{-1} * z
      L_inv_z <- solve(L, z)  
      
      # Compute the quadratic term: (L^{-1}z)^T (L^{-1}z)
      quad_term <- sum(L_inv_z^2)
      
      # Compute the log-likelihood
      ll <- -0.5 * (n * log(2 * pi) + log_det_term + quad_term)
      
      return(ll)
  })

  
  return(sum(ll))
}

# MAP (under uniform prior)
MAP <- function(Z1, theta_0) {
  neg_log_lik <- function(theta, Z) -log_lik(theta, Z)
  optim(theta_0, neg_log_lik, Z = Z1, method = "Brent", lower = 0, upper = 0.5)$par
}

# map_estimates <- sapply(Z_test, MAP, theta_0 = 0.25)
map_estimates <- unlist(mclapply(Z_test, MAP, theta_0 = 0.25, mc.cores = detectCores() - 1))
map_rmse <- sqrt(mean((theta_test - map_estimates)^2))
saveRDS(map_rmse, file.path(savepath, "map_rmse.rds"))
```


```{r}
## Visualise performance of each architecture and the ensemble
results <- function(estimators) {
  ensemble <- Ensemble(estimators)
  assessment <- assess(
    c(estimators, list(ensemble)), 
    theta_test, 
    Z_test, 
    parameter_names = "Î¸",
    estimator_names = c(paste("Estimator", 1:J), "Ensemble")
  )
  df <- rmse(assessment)
  ensemble_rmse <- df$rmse[df$estimator == "Ensemble"]
  df$difference <- df$rmse - ensemble_rmse 
  df$relative_difference <- df$difference  / ensemble_rmse
  return(df)
}

dfs <- lapply(seq_along(all_estimators), function(i) {
  df <- results(all_estimators[[i]])
  df$architecture <- as.character(i)
  return(df)
})
df <- do.call(rbind, dfs)
write.csv(df, file = "df_architecture.csv", row.names = FALSE)

modify_arch <- function(df) {
  df <- df %>% filter(architecture != 2)
  df <- df %>%
    mutate(architecture = case_when(
      architecture == 3 ~ 2,  # Change 3 to 2
      architecture == 4 ~ 3,  # Change 4 to 3
      TRUE ~ architecture     # Keep all other values the same
    ))
  df$architecture <- as.character(df$architecture)
  df
}

df <- modify_arch(df)

gg <- ggplot(df) + 
  geom_boxplot(aes(x = architecture, y = rmse, group = architecture)) + 
  geom_point(data = df %>% filter(estimator == "Ensemble"), 
             aes(x = architecture, y = rmse),  
             colour = "red") + 
  geom_hline(yintercept = map_rmse, lty = "dashed") + 
  labs(x = "Architecture", y = "RMSE") + 
  theme_bw()


## Visualise performance as a function of number of ensemble components
results2 <- function(estimators) {
  ensemble <- Ensemble(estimators)
  assessment <- assess(
    lapply(1:J, function(j) Ensemble(estimators[1:j])), 
    theta_test, 
    Z_test, 
    parameter_names = "Î¸",
    estimator_names = paste("Ensemble with", 1:J, "components")
  )
  df <- rmse(assessment)
  df$j <- as.numeric(gsub("[^0-9]", "", df$estimator))
  return(df)
}

dfs2 <- lapply(seq_along(all_estimators), function(i) {
  df <- results2(all_estimators[[i]])
  df$architecture <- as.character(i)
  return(df)
})
df2 <- do.call(rbind, dfs2)
write.csv(df2, file = "df_ensemblecomponents.csv", row.names = FALSE)

df2 <- modify_arch(df2)

gg2 <- ggplot(df2, aes(x=j, y=rmse, group = architecture, lty = architecture)) + 
  geom_point() + 
  geom_line() + 
  theme_bw() + 
  labs(x = "Number of ensemble components", y = "RMSE", lty = "Architecture") + 
  scale_x_continuous(breaks = 1:J) +  # Only integer breaks
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(), 
        legend.key.width = unit(0.5, "cm"))

figure <- egg::ggarrange(gg, gg2, nrow = 1)
ggsv <- function(filename, plot, ...) {
  suppressWarnings({
    for (device in c("pdf", "png")) {
      ggsave(plot, file = paste0(filename, ".", device), device = device, ...)
    }
  })
}
ggsv(file = file.path(savepath, "ensemble"), plot = figure, width = 7.4, height = 3.25)

```

# Different grid sizes 

```{r}
ResNet <- all_ensembles[[length(all_ensembles)]]

# Train the ResNet with varying grid sizes 
simulate_variable_grid <- function(...) {
  N <- sample(8:16, 2)
  simulate(N = N, ...)
}
K <- 50000
set.seed(1)
theta_train <- prior(K)          
theta_val   <- prior(K/10) 
Z_train <- apply(theta_train, 2, function(theta) simulate_variable_grid(as.matrix(theta))[[1]]) 
Z_val <- apply(theta_val, 2, function(theta) simulate_variable_grid(as.matrix(theta))[[1]]) 

ResNet <- train(
        ResNet,
        theta_train = theta_train,
        theta_val = theta_val,
        Z_train = Z_train,
        Z_val = Z_val, 
        savepath = file.path(savepath, "ResNet_variable_grids"), 
        epochs = 3
      )

```

```{r}
library(ggplot2)
library(reshape2)

# Example list of matrices
matrix_list <- list(
  matrix1 = matrix(runif(100), nrow = 10, ncol = 10),
  matrix2 = matrix(runif(400), nrow = 20, ncol = 20)
)

# Function to convert matrix to long format data frame
convert_to_df <- function(mat, mat_name) {
  df <- melt(mat)  # Converts matrix to data frame with Var1, Var2, value
  colnames(df) <- c("x", "y", "value")  # Rename columns
  df$matrix <- mat_name  # Add matrix name to distinguish between different matrices
  return(df)
}

heatmaps <- function(matrix_list) {
  
  matrix_list <- lapply(matrix_list, drop)
  
  # Convert each matrix in the list to long format
  df_list <- lapply(names(matrix_list), function(name) convert_to_df(matrix_list[[name]], name))

  # Combine the data frames for all matrices
  df_combined <- do.call(rbind, df_list)
  
  # Plot the heatmaps
  ggplot(df_combined, aes(x = x/16, y = y/16, fill = value)) +
    geom_tile() +
    facet_wrap(~ matrix, nrow=1) +  # Facet by matrix, keep different scales
    coord_fixed() +  # Maintain aspect ratio
    theme_bw() +
    scale_fill_viridis_c(option = "magma") + 
      theme(
      # strip.background = element_blank(),
      # strip.text.x = element_blank(),
      legend.position = "none"
    ) +
    labs(fill = "Z", x = expression(s[1]), y = expression(s[2])) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) 
}


```

```{r}
B <- 100
theta <- matrix(0.1)

convert_to_list <- function(Z) {
  lapply(1:dim(Z)[4], function(i) Z[, , , i, drop = FALSE])  
}

# NB doesn't work so well with: c(7, 13), c(11, 8), c(8, 8), c(17, 17), c(18, 18), 
# Should investigate whether this is a general trend, that the estimator needs 
# to be trained with grids that are at least as large as those that it will be 
# used with in practice. 
N_list <- list(c(16, 16), c(16, 24), c(24, 24), c(32, 32))
Z_list <- lapply(N_list, function(N) {
  simulate(theta, m = B, N = N)[[1]] %>% convert_to_list
})
thetahat_list <- lapply(Z_list, function(Z) {
  c(estimate(ResNet, Z))
})

matrix_list <- lapply(Z_list, function(Z) Z[[1]])
N_vector <- sapply(N_list, function(N) paste(N[1], "x", N[2], sep = ""))
names(matrix_list) <- N_vector
gg_data <- heatmaps(matrix_list)

df <- data.frame(
  N = rep(as.character(N_vector), each = B),
  thetahat = unlist(thetahat_list), 
  theta = c(theta)
)

gg_estimates <- ggplot(df) + 
  geom_boxplot(aes(x = N, y = thetahat, group = N)) + 
  # facet_wrap(~N, nrow = 1) + 
  geom_hline(yintercept = theta, lty = "dashed") + 
  labs(x = "", y = "Estimate") + 
  theme_bw()


figure <- ggpubr::ggarrange(gg_data, gg_estimates, nrow = 2, heights = c(1.5, 1))

ggsv(file = file.path(savepath, "varying_grids"), plot = figure, width = 8.54, height = 4.41)

# gg_estimates <- ggplot(df) + 
#   geom_boxplot(aes(x = N, y = thetahat, group = N)) + 
#   facet_wrap(~N, nrow = 1, scales = "free_x") + 
#   geom_hline(yintercept = theta, lty = "dashed") + 
#   labs(x = "", y = "Estimate") + 
#   theme_bw() + 
#   theme(
#       strip.background = element_blank(),
#       strip.text.x = element_blank(),
#       panel.grid = element_blank(), 
#       axis.text.x = element_blank(), 
#       axis.ticks.x = element_blank()
#   )
```












