using LinearAlgebra
using NeuralEstimators
using Optim
using Folds
using Flux: flatten
using RecursiveArrayTools
using Bessels: besselk

function MAP(Z::V, Î¾) where {T, N, A <: AbstractArray{T, N}, V <: AbstractVector{A}}

	# Compress the data from an n-dimensional array to a matrix
	Z = flatten.(Z)

	# Convert to Float64 to avoid rounding errors
	Z  = broadcast.(x -> !ismissing(x) ? Float64(x) : identity(x), Z)
	Î¸â‚€ = Float64.(Î¾.Î¸â‚€)

	# If Z is replicated, try to replicate Î¸â‚€ accordingly
	K = size(Î¸â‚€, 2)
	m = length(Z)
	if m != K
		if (m Ã· K) == (m / K)
			Î¸â‚€ = repeat(Î¸â‚€, outer = (1, m Ã· K))
		else
			error("The length of the data vector, m = $m, and the number of parameter configurations, K = $K, do not match; further, m is not a multiple of K, so we cannot replicate Î¸ to match Z.")
		end
	end

	# Optimise
	Î¸ = Folds.map(eachindex(Z)) do k
		 MAP(Z[k], Î¸â‚€[:, k], Î¾)
	end

	# Convert to matrix
	Î¸ = hcat(Î¸...)

	return Î¸
end

function MAP(Z::M, Î¸â‚€::V, Î¾) where {T, R, V <: AbstractVector{T}, M <: AbstractMatrix{R}}

    # Since we transform the parameters during optimisation to force the estimates
    # to be within the prior support, here we compute the inverse-transformed true
    # values, which will then be passed into the optimiser.
	Î© = [Î¾.Î©...]
	Î¸â‚€[Î¾.nonÎ£idx] = scaledlogit.(Î¸â‚€[Î¾.nonÎ£idx], Î©[Î¾.nonÎ£idx])

	# Closure that will be minimised
	loss(Î¸) = nll(Î¸, Z, Î¾)

	# Estimate the parameters
	Î¸ = optimize(loss, Î¸â‚€, NelderMead()) |> Optim.minimizer

	# Convert estimates to the original scale
	# Since we parameterised Î£ directly in terms of its Cholesky factor,
	# we need to now construct Î£ and extract the implied parameters.
	Î¸[Î¾.nonÎ£idx] = scaledlogistic.(Î¸[Î¾.nonÎ£idx], Î©[Î¾.nonÎ£idx])
	vectocholesky = Î¾.correlations_only ? vectocorrelationcholesky : vectocovariancecholesky
	L = vectocholesky(Î¸[Î¾.Î£idx])
	Î£ = L * L'
	Î¸[Î¾.Î£idx] = Î£[tril(trues(Î¾.d, Î¾.d), Î¾.correlations_only ? -1 : 0)]

	return Î¸
end

function nll(Î¸, Z, Î¾)

    # Constrain Î¸ to be within the prior support and valid
    Î© = [Î¾.Î©...]
    Î¸ = copy(Î¸)
    Î¸[Î¾.nonÎ£idx] = scaledlogistic.(Î¸[Î¾.nonÎ£idx], Î©[Î¾.nonÎ£idx])
    vectocholesky = Î¾.correlations_only ? vectocorrelationcholesky : vectocovariancecholesky
	L = vectocholesky(Î¸[Î¾.Î£idx])

	d  = size(Z, 1)
	Î³  = Î¸[findfirst(parameter_names .== "Î³")]; Î³ = repeat([Î³], d)
	Ï‰  = Î¸[findfirst(parameter_names .== "Ï‰")]
	Î·  = one(eltype(Î¸)) 
	Î»  = Î¸[findfirst(parameter_names .== "Î»")]

	# Compute the log-likelihood
	â„“ = GHdensity(Z, L, Î³ = Î³, Î» = Î», Ï‰ = Ï‰, Î· = Î·)

	return -â„“
end

# Based on the approach described [here](https://stats.stackexchange.com/a/404453).
# The mappping of `v` to the Cholesky factor is a well behaved diffeomorphism,
# so it can be inverted; however, this inversion is not implemented.
"""
	vectocorrelationcholesky(v::Vector)
Transforms a vector `v` âˆˆ â„áµˆ, where d is a triangular number, into the Cholesky
factor ğ‹ of a correlation matrix ğ‘ = ğ‹ğ‹'.
"""
function vectocorrelationcholesky(v)
	L = vectotril(v; strict = true)
	L = unitdiagonal(L)
	x = rowwisenorm(L)
	L = L ./ x
	L = LowerTriangular(L)
	return L
end
unitdiagonal(A) = A - Diagonal(A[diagind(A)]) + I

# # Alternative vectocorrelationcholesky that Stan uses (I prefer the simpler version above)
# function vectocorrelationcholesky(v)
# 	v = cpu(v)
# 	z = tanh.(vectotril(v; strict=true))
# 	n = length(v)
# 	d = (-1 + isqrt(1 + 8n)) Ã· 2 + 1
# 	L = [ correlationcholeskyterm(i, j, z)  for i âˆˆ 1:d, j âˆˆ 1:d ]
# end
# function correlationcholeskyterm(i, j, z)
# 	T = eltype(z)
# 	if i < j
# 		zero(T)
# 	elseif 1 == i == j
# 		one(T)
# 	elseif 1 == j < i
# 		z[i, j]
# 	elseif 1 < j == i
# 		prod(sqrt.(one(T) .- z[i, 1:j-i].^2))
# 	else
# 		z[i, j] * prod(sqrt.(one(T) .- z[i, 1:j-i].^2))
# 	end
# end

# Based on Williams (1996) "Using Neural Networks to Model Conditional Multivariate Densities"
"""
	vectocovariancecholesky(v::Vector)
Transforms a vector `v` âˆˆ â„áµˆ, where d is a triangular number, into the Cholesky
factor ğ‹ of a covariance matrix ğšº = ğ‹ğ‹'.
"""
function vectocovariancecholesky(v)
	L = vectotril(v)
	diag = L[diagind(L)]
	L = L - Diagonal(diag) + Diagonal(softplus.(diag))
	L = LowerTriangular(L)
	return L
end


