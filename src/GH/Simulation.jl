using NeuralEstimators
using LinearAlgebra
using Distributions
using Distances: pairwise, Euclidean
using Folds
using Statistics
using Test
using Random: seed!

import NeuralEstimators: simulate

# Prior
Î© = (
	Î³ = Uniform(-0.3, 0.3),
	Ï‰ = Uniform(0.1, 1),
	Î» = Uniform(-1, 1),
	Ï = Uniform(0.05, 0.35),
	Î½ = Uniform(0.5, 2.0)
)
parameter_names = String.(collect(keys(Î©)))

# Spatial domain
pts = range(0.0, 1.0, 16)
S = expandgrid(pts, pts)
D = pairwise(Euclidean(), S, S, dims = 1)

# Tuple containing key information
Î¾ = (
	Î© = Î©, pts = pts, S = S, D = D, p = length(Î©), d = size(S, 1),
	parameter_names = parameter_names,
	Ï_idx = findfirst(parameter_names .== "Ï"),
	Î½_idx = findfirst(parameter_names .== "Î½")
)


# ---- Parameter configurations ----

struct Parameters{T, V, I} <: ParameterConfigurations
	Î¸::Matrix{T}
	chols::V
	chol_pointer::Vector{I} # chol_pointer[i] gives the Cholesky factor associated with Î¸[:, i]
end

function Parameters(K::Integer, Î¾; J::Integer = 1)

	# All parameters not associated with the Gaussian process
	Î¸ = [rand(Ï‘, K * J) for Ï‘ in drop(Î¾.Î©, (:Ï, :Î½))]

	# Determine if we are estimating Î½
	estimate_Î½ = "Î½" âˆˆ Î¾.parameter_names

	# Covariance parameters
	Ï = rand(Î¾.Î©.Ï, K)
	Î½ = estimate_Î½ ? rand(Î¾.Î©.Î½, K) : fill(Î¾.Î½, K)
	chols = maternchols(Î¾.D, Ï, Î½)
	Ï = repeat(Ï, inner = J)
	Î½ = repeat(Î½, inner = J)
	chol_pointer = repeat(1:K, inner = J)

	# Now insert Ï (and possibly Î½) into Î¸.
	Î¸â‚ = Î¸[1:(Î¾.Ï_idx-1)]
	Î¸â‚‚ = Î¸[Î¾.Ï_idx:end] # Note that Ï and Î½ are not in Î¸, so we don't need to skip any indices.
	if estimate_Î½
		@assert Î¾.Î½_idx == Î¾.Ï_idx + 1 "We assume that Ï and Î½ are stored continguously in Î¸, that is, that Î¾.Î½_idx = Î¾.Ï_idx + 1"
		Î¸ = [Î¸â‚..., Ï, Î½, Î¸â‚‚...]
	else
		Î¸ = [Î¸â‚..., Ï, Î¸â‚‚...]
	end

	# Concatenate into a matrix
	Î¸ = hcat(Î¸...)'
	Î¸ = Float32.(Î¸)

	Parameters(Î¸, chols, chol_pointer)
end


# ---- Marginal simulation ----

function simulate(parameters::Parameters, m::R) where {R <: AbstractRange{I}} where I <: Integer

	K = size(parameters, 2)
	mÌƒ = rand(m, K)

	# Extract the parameters from Î¸, and the Cholesky factors
	Î¸ = parameters.Î¸
	T = eltype(Î¸)
	Î³ = Î¸[findfirst(parameter_names .== "Î³"), :]
	Î» = Î¸[findfirst(parameter_names .== "Î»"), :]
	Ï‰ = Î¸[findfirst(parameter_names .== "Ï‰"), :]
	chols = parameters.chols
	chol_pointer = parameters.chol_pointer

	# infer the number of observations
	d = size(chols, 1)
	@assert âˆš(d) == isqrt(d) "This code assume that the spatial domain is a square"

	Z = Folds.map(1:K) do k
		L = view(chols, :, :, chol_pointer[k])
		z = simulateGH(L, mÌƒ[k]; Î¼ = zero(T), Î³ = Î³[k], Î» = Î»[k], Ï‰ = Ï‰[k])
		z = Float32.(z)
		z = reshape(z, isqrt(d), isqrt(d), 1, :)
		z
	end

	return Z
end
simulate(parameters::Parameters, m::Integer) = simulate(parameters, range(m, m))
simulate(parameters::Parameters) = stackarrays(simulate(parameters, 1))

# ---- Conditional simulation ----

"""
Generic function for defining conditional simulation, used in the EM algorithm.

The user must provide a method:

	simulateconditional(Zâ‚::M, Î¸; nsims::Integer = 1, kwargs...) where {M <: AbstractMatrix{Union{Missing, T}}} where T

The two positional arguments are:

- `Zâ‚`: the observed, incomplete data. The last dimension of `Zâ‚` contains the replicates; the other dimensions store the response variable.
- `Î¸`: vector of parameters used for conditional simulation.

The argument `nsims` controls the number of simulations.
"""
function simulateconditional end

function simulateconditional(Z::A, Î¸; nsims::Integer = 1, kwargs...) where {A <: AbstractArray{Union{Missing, T}, N}} where {T, N}

	# Save the original dimensions
	dims = size(Z)

	# Convert to matrix and pass to the matrix method
	Z = simulateconditional(Flux.flatten(Z), Î¸; nsims = nsims, kwargs...)

	# Convert Z to the correct dimensions
	Z = reshape(Z, dims[1:end-1]..., :)

	return Z
end

function simulateconditional(Z::V, Î¸; nsims::Integer, kwargs...) where {V <: AbstractVector{Union{Missing, T}}} where T

	## Convert to matrix and pass to the matrix method
	Z = reshape(Z, (length(Z), 1))
	Z = simulateconditional(Z, Î¸; nsims = nsims, kwargs...)

	return Z
end


"""
Replicates are stored in the columns of `Z`
"""
function simulateconditional(Z::M, Î¸; nsims::Integer, Î¾) where {M <: AbstractMatrix{Union{Missing, T}}} where T

	Î¸ = Float64.(Î¸)
	
	# Extract objects from Î¾
	D = Î¾.D # distance matrix 
	d = size(D, 1)

	# Extract the parameters from Î¸ using information in Î¾
	parameter_names = Î¾.parameter_names
	Î³ = Î¸[findfirst(parameter_names .== "Î³")]; Î³ = repeat([Î³], d)
	Î¼ = repeat([0], length(Î³))
	Î· = one(eltype(Î¸)) 
	Ï‰ = Î¸[findfirst(parameter_names .== "Ï‰")]
	Î» = Î¸[findfirst(parameter_names .== "Î»")]
	Ï = Î¸[findfirst(parameter_names .== "Ï")]
	Î½ = Î¸[findfirst(parameter_names .== "Î½")]
	
	# Compute covariance matrix
	Î£ = matern.(UpperTriangular(D), Ï, Î½)
	Î£ = Symmetric(Î£)

	# Remove any columns that contain only missing elements
	Z = Z[:, vec(mapslices(z -> !all(ismissing.(z)), Z, dims = 1))]

	# Conditional simulation
	simulateconditionalGH(Z, Î£; Î³ = Î³, Î¼ = Î¼, Î» = Î», Ï‰ = Ï‰, Î· = Î·, H = nsims)
end


# ---- Generalised-hyperbolic simulation ----

# Marginal simulation is easy, we just need to simulate from a Gaussian process
# and from a generalised inverse Gaussian (GIG) distribution.

function simulateGH(L::AbstractArray{T, 2}; Î¼, Î³, Î», Ï‰, Î· = 1) where T <: Number
	W = simulategaussianprocess(L)
	M = simulategig(Î» = Î», Ï‰ = Ï‰, Î· = Î·)
	Z = Î¼ .+ Î³ * M .+ sqrt(M) * W
	return Z
end

function simulateGH(L::AbstractArray{T, 2}, m::Integer; args...) where T <: Number
	n = size(L, 1)
	Z = similar(L, n, m)
	for h âˆˆ 1:m
		Z[:, h] = simulateGH(L; args...)
	end
	return Z
end

"""
	simulateconditionalGH(Z::Matrix{Union{Missing, T}, Î£::Matrix; Î³, Î¼, Î», Ï‰, Î·, H::Integer) where T
Conditional simulation from the generalised hyperbolic distribution, described
below.

The data Z should be a dÃ—n matrix of incompletely observed replicates.
The return type is a dÃ—n`H` matrix obtained by conditionally simulating `H`
times for each replicate (note that the order of replicates is not preserved).

# GH distribution

The d-dimensional random variable ğ™ is called a normal mean-variance mixture
(NMVM) if it can be represented as,

```math
ğ™ = ğ› + Mğ›„ + Mğ•
```

where ğ› âˆˆ â„áµˆ and ğ›„ âˆˆ â„áµˆ, and where M is a positive mixing random variable that
is independent of ğ• âˆ¼ Gau(ğŸ, ğšº) for covariance matrix ğšº. This flexible family of
distributions is closed under conditioning. With ğ™ = (ğ™â‚', ğ™â‚‚')' and with ğ›, ğ›„,
and ğšº partitioned accordingly, ğ™â‚‚ | ğ™â‚ is also a NMVM, with mixing variable
M | ğ™â‚ and parameters (Jamalizadeh and Balakrishnan, 2019, Thm. 1),

```math
ğ›â‚‚âˆ£â‚  = ğ›â‚‚ + ğšºâ‚‚â‚ğšºâ‚â‚â»Â¹(ğ™â‚âˆ’ğ›â‚),
ğ›„â‚‚|â‚  = ğ›„â‚‚ - ğšºâ‚‚â‚ğšºâ‚â‚â»Â¹ğ›„â‚,
ğšºâ‚‚â‚‚âˆ£â‚ = ğšºâ‚‚â‚‚ - ğšºâ‚‚â‚ğšºâ‚â‚â»Â¹ğšºâ‚â‚‚.
```

The generalised hyperbolic (GH) distribution (Barndorff-Nielsen, 1977) is
obtained when M follows a generalised inverse Gaussian (GIG) distribution.
If M âˆ¼ GIG(Ï‰, Î·, Î») with density function,

```math
f(m; Î», Î·, Ï‰) âˆ m^{Î»-1}e^{-Ï‰(Î·/m + m/Î·)/2}, 	m > 0,
```

then M | Zâ‚ also follows a GIG distribution with parameters
(Jamalizadeh and Balakrishnan, 2019, Cor. 2),

```math
Ï‰â‚‚âˆ£â‚ = [(q(ğ™â‚; ğ›â‚, ğšºâ‚â‚) + Ï‰Î·)(Ï‰/Î· + ğ›„â‚'ğšºâ‚â‚â»Â¹ğ›„â‚)]^{1/2},
Î·â‚‚âˆ£â‚ = [(q(ğ™â‚; ğ›â‚, ğšºâ‚â‚) + Ï‰Î·)/(Ï‰/Î· + ğ›„â‚'ğšºâ‚â‚â»Â¹ğ›„â‚)]^{1/2},
Î»â‚‚âˆ£â‚ = Î» âˆ’ dim(Zâ‚)/2,
```

where ``q(ğ™â‚; ğ›â‚, ğšºâ‚â‚) = (ğ™â‚âˆ’ğ›â‚)'ğšºâ‚â‚â»Â¹(ğ™â‚âˆ’ğ›â‚)``.

# Examples
```
d = 5
n = 1000
Z = rand(d, n)
Z = removedata(Z, 0.5)

Î¼ = randn(d)
Î³ = randn(d)
Î£ = randn(d, d); Î£ = Symmetric(0.5 * (Î£ + Î£')); Î£ = Î£ +  d * I(d)
Î» = randn(1)[1]
Ï‰ = rand(1)[1]
Î· = rand(1)[1]

simulateconditionalGH(Z, Î£; Î³ = Î³, Î» = Î», Ï‰ = Ï‰, Î· = Î·, Î¼ = Î¼, H = 10)
```
"""
function simulateconditionalGH(Z::Mâ‚, Î£::Mâ‚‚; Î³, Î¼, Î», Ï‰, Î·, H::Integer) where {Mâ‚ <: AbstractMatrix{Union{Missing, T}},  Mâ‚‚ <: AbstractMatrix} where T

  	# Dimension of the response and number of replicates
  	d  = size(Z, 1)
  	n  = size(Z, 2)

  	# Get the indices of the observed component for each replicate.
	idx = [findall(x -> !ismissing(x), vec(Z[:, i])) for i âˆˆ 1:n]

	# Handle replicates without any observations, and completely observed replicates
	unique_idx = unique(idx)
	@assert !any(length.(unique_idx) == 0) "dim(Zâ‚) = 0: cannot do conditional simulation without data to condition on" # NB could just simulate from the marginal in this case
	deleteat!(unique_idx, findall(Ref(1:d) .== unique_idx))

	# Consider groups of replicates with the same missingness pattern for computational efficiency
	W = map(unique_idx) do Iâ‚

    	# Missing elements
    	Iâ‚‚ = (1:d)[1:d .âˆ‰ Ref(Iâ‚)]

    	# Extract replicates with the missingness pattern Iâ‚
    	z =  Z[:, findall(Ref(Iâ‚) .== idx)]
		nÌƒ = size(z, 2)

		# Extract parameter subvectors and submatrices
		Î¼â‚ = Î¼[Iâ‚]
		Î¼â‚‚ = Î¼[Iâ‚‚]
		Î³â‚ = Î³[Iâ‚]
		Î³â‚‚ = Î³[Iâ‚‚]
		Î£â‚â‚ = Î£[Iâ‚, Iâ‚]
		Î£â‚‚â‚‚ = Î£[Iâ‚‚, Iâ‚‚]
		Î£â‚‚â‚ = Î£[Iâ‚‚, Iâ‚]
		Î£â‚â‚‚ = Î£[Iâ‚, Iâ‚‚]

		# Compute conditional parameters that do not depend on Zâ‚
		Lâ‚â‚ = try
			cholesky(Symmetric(Î£â‚â‚)).L
		catch
			@warn "Failed to compute the Cholesky factor of Î£â‚â‚: ignoring this batch of $nÌƒ out of $n total replicates"
			return missing
		end
		u = Lâ‚â‚ \ Î£â‚â‚‚   # Lâ‚â‚â»Â¹Î£â‚â‚‚
		w = Lâ‚â‚ \ Î³â‚    # Lâ‚â‚â»Â¹Î³â‚
		quadform_Î³Î³ = w'w
		Î»star   = Î» - length(Iâ‚)/2
		Î³â‚‚star  = Î³â‚‚ - u'w
		Î£â‚‚â‚‚star = Î£â‚‚â‚‚ - u'u
		Lâ‚‚â‚‚star = try
			cholesky(Symmetric(Î£â‚‚â‚‚star)).L
		catch
			@warn "Failed to compute the Cholesky factor of Î£â‚‚â‚‚âˆ£â‚: ignoring this batch of $nÌƒ out of $n total replicates"
			return missing
		end

		# Conditional simulation for each Zâ‚
		W = map(eachcol(z)) do Zâ‚

			# Extract the observed data and drop Missing from the eltype
			Zâ‚ = Zâ‚[Iâ‚]
			Zâ‚ = [Zâ‚...]

			# Compute conditional parameters that depend on Zâ‚
			v = Lâ‚â‚ \ (Zâ‚ - Î¼â‚) # Lâ‚â‚â»Â¹(Zâ‚ - Î¼â‚)
			Î¼â‚‚star  = Î¼â‚‚ + u'v
			quadform_zz = v'v
			Ï‰star = try
				sqrt( (Ï‰*Î· + quadform_zz) * (Ï‰/Î· + quadform_Î³Î³) )
			catch
				@warn "Computation of Ï‰â‚‚âˆ£â‚ failed: ignoring this batch of $nÌƒ out of $n total replicates"
				return missing
			end
			Î·star = try
				sqrt( (Ï‰*Î· + quadform_zz) / (Ï‰/Î· + quadform_Î³Î³) )
			catch
				@warn "Computation of Î·â‚‚âˆ£â‚ failed: ignoring this batch of $nÌƒ out of $n total replicates"
				return missing
			end

			# Simulate from the conditional distribution Zâ‚‚ âˆ£ Zâ‚, Î¸
			Zâ‚‚ = simulateGH(Lâ‚‚â‚‚star, H; Î¼ = Î¼â‚‚star, Î³ = Î³â‚‚star, Î» = Î»star, Ï‰ = Ï‰star, Î· = Î·star)

			# Combine the observed and missing data to form the complete data
			W = map(1:H) do h
				w = Vector{T}(undef, d)
				w[Iâ‚] = Zâ‚
				w[Iâ‚‚] = Zâ‚‚[:, h]
				w
			end
			hcat(W...) # d Ã— H Matrix{T}
		end
		W = skipmissing(W)
		W = hcat(W...) # d Ã— (náµ¢H) Matrix{T}, where náµ¢ is the number of replicates in the current batch
		if all(ismissing.(W)) W =  Matrix{T}(undef, d, 0) end # when all simulations fails
		W
	end
	W = skipmissing(W)
	W = hcat(W...) # d Ã— (nÌƒH) Matrix{T}, where nÌƒ is the total number of replicates excluding those replicates that are fully observed
  if all(ismissing.(W)) W =  Matrix{T}(undef, d, 0) end # when all simulations fails

	# Now add the completely observed replicates (if there are any)
	if any(idx .== Ref(1:d))
		z =  Z[:, findall(idx .== Ref(1:d))] # extract completely observed replicates
		w = repeat(z, inner = (1, H))        # repeat each replicate H times
		w = convert(Matrix{T}, w)            # drop Missing from the eltype
		W = hcat(W, w) # d Ã— (nH) Matrix{T}
	end

	return W # d Ã— (nH) Matrix{T}
end

function simulateconditionalGH(Z::V, Î£::Mâ‚‚; args...) where {V <: AbstractVector{Union{Missing, T}},  Mâ‚‚ <: AbstractMatrix} where T
	Z = reshape(Z, :, 1)
	simulateconditionalGH(Z, Î£; args...)
end


# ---- Generalised Inverse Gaussian (GIG) ----

# This code was adapted from the following Distributions.jl pull request:
# https://github.com/JuliaStats/Distributions.jl/pull/1300/files

using Distributions
import Distributions: convert
import Base: rand

"""
	simulategig(n = 1; Ï‰, Î·, Î»)
Simulates `n` realisations of a generalized inverse Gaussian (GIG) random
variable with density,

```math
f(x; Î», Î·, Ï‰)  âˆ x^{Î»-1}e^{-Ï‰(Î·/x + x/Î·)/2}, x > 0,
```

for concentration parameter Ï‰ > 0, scale parameter Î· > 0, and shape
parameter Î» âˆˆ â„. Note that this is equivalent to the parameterisation,

```math
f(x; Î», Ï‡, Ïˆ)  âˆ x^{Î»-1}e^{-(Ï‡/x + Ïˆx)/2}
```

with Ï‡ = Ï‰Î· and Ïˆ = Ï‰/Î·. Note that the inverse parameterisation is Î· = âˆš{Ï‡/Ïˆ} and Ï‰ = âˆš{Ï‡Ïˆ}.

The sampling procedure is based on [HÃ¶rmann & Leydold (2014)](https://doi.org/10.1007/s11222-013-9387-3).
"""
function simulategig(n::Integer; Î», Ï‰, Î·)
	Ïˆ = Ï‰/Î·
	Ï‡ = Ï‰ * Î·
	rand(GeneralisedInverseGaussian(Ïˆ, Ï‡, Î»), n)
end
function simulategig(; Î», Ï‰, Î·)
	Ïˆ = Ï‰/Î·
	Ï‡ = Ï‰ * Î·
	rand(GeneralisedInverseGaussian(Ïˆ, Ï‡, Î»))
end

struct GeneralisedInverseGaussian{T<:Real} <: ContinuousUnivariateDistribution
    Ïˆ::T
    Ï‡::T
    p::T #NB p âŸº Î»
    GeneralisedInverseGaussian{T}(Ïˆ::T, Ï‡::T, p::T) where T = new{T}(Ïˆ,Ï‡,p)
end

function GeneralisedInverseGaussian(Ïˆ::T,Ï‡::T,p::T) where {T<:Real}
    @assert Ïˆ > zero(Ïˆ) && Ï‡ > zero(Ï‡)
    return GeneralisedInverseGaussian{T}(Ïˆ,Ï‡,p)
end

GeneralisedInverseGaussian(Ïˆ::Real, Ï‡::Real, p::Real) = GeneralisedInverseGaussian(promote(Ïˆ,Ï‡,p)...)
GeneralisedInverseGaussian(Ïˆ::Integer, Ï‡::Integer, p::Integer) = GeneralisedInverseGaussian(float(Ïˆ), float(Ï‡), float(p))
function convert(::Type{GeneralisedInverseGaussian{T}}, Ïˆ::Real, Ï‡::Real, p::Real) where T<:Real
    GeneralisedInverseGaussian(T(Ïˆ),T(Ï‡),T(p))
end
function convert(::Type{GeneralisedInverseGaussian{T}}, d::GeneralisedInverseGaussian{S}) where {T <: Real, S<: Real}
    GeneralisedInverseGaussian(T(d.Ïˆ), T(d.Ï‡), T(d.p))
end
params(d::GeneralisedInverseGaussian) = (d.Ïˆ, d.Ï‡, d.p)
@inline partype(d::GeneralisedInverseGaussian{T}) where {T<:Real} = T

#### Sampling

# NB Added iteration limit in the while loops to prevent hanging when the
# parameters are extreme

rand(d::GeneralisedInverseGaussian, n::Int64) = [rand(d) for _ in 1:n]

function rand(d::GeneralisedInverseGaussian)
    (Ïˆ,Ï‡,p) = params(d)
    Ï‰ = sqrt(Ïˆ/Ï‡) # NB This is not the same Ï‰ as described above
    Î· = sqrt(Ïˆ*Ï‡) # NB This is not the same Î· as described above
    Î» = abs(p)
    if Î· > 1 || Î» > 1
        x = sample_unif_mode_shift(Î»,Î·)
    else
        Î·_bound = min(1/2, (2/3)*sqrt(1 - p))
        if Î· < 1 && Î· >= Î·_bound
            x = sample_unif_no_mode_shift(Î»,Î·)
        elseif Î· < Î·_bound && Î· > 0
            x = concave_sample(Î»,Î·)
        else
            throw(ArgumentError("None of the required conditions on the parameters are satisfied"))
        end
    end
    if p >= 0
        return x/Ï‰
    else
        return 1 / (Ï‰*x)
    end
end

#Sample the 2-parameter GIG distribution with parameters p and Î· using the Rejection method
#as described by HÃ¶rmann & Leydold (2014).
function concave_sample(p::Real, Î·::Real; max_iteration::Integer = 10000, verbose::Bool = false)

    Ïµ = typeof(Î·)(0.000001) # small positive constant to prevent log(<0)

    m = Î·/((1 - p) + sqrt((1 - p)^2 + Î·^2))
    x_naut = Î·/(1 - p)
    x_star = max(x_naut,2/Î·)
    k1 = g(m,p,Î·)
    A1 = k1 * x_naut
    k2 = 0
    A2 = 0
    if x_naut < 2/Î·
        k2 = exp(-Î·)
        if p > 0
            A2 = k2 * ((2/Î·)^p - x_naut^p) / p
        else
            y = 2/Î·^2
            if y < 0 y += Ïµ end
            A2 = k2 * log(y)
        end
    end
    k3 = x_star^(p-1)
    A3 = 2 * k3 * exp(-x_star * Î· / 2) / Î·
    A = A1 + A2 + A3

    iteration = 1
    while true

        u = rand(Uniform(0,1))
        v = rand(Uniform(0,1))*A
        h = Inf
        if v <= A1
            x = x_naut * v / A1
            h = k1
        elseif v <= A1 + A2
            v = v - A1
            if p > 0
                x = (x_naut^p + (v*p/k2))^(1/p)
            else
                x = Î·*exp(v * exp(Î·))
            end
            h = k2 * x^(p-1)
        else
            v = v - (A1 + A2)
            y = exp(-x_star * Î· / 2) - (v * Î·) / (2 * k3)
            if y < 0 y += Ïµ end
            x = -2 * log(y) / Î·
            h = k3 * exp(-x * Î· / 2)
        end
        if (u * h) <= g(x,p,Î·) || iteration == max_iteration
            iteration == max_iteration && verbose && @warn "GIG sampling reached max iteration: check results carefully"
            return x
        end
        iteration += 1
    end
end

#Sample the 2-parameter GIG distribution with parameters p and Î· using the
#Ratio-of-Uniforms without mode shift as described by HÃ¶rmann & Leydold (2014).
function sample_unif_no_mode_shift(p::Real,Î·::Real; max_iteration::Integer = 10000, verbose::Bool = false)
    m = Î·/((1 - p) + sqrt((1 - p)^2 + Î·^2))
    xâº= ((1 + p) + sqrt((1 + p)^2 + Î·^2))/Î·
    vâº= sqrt(g(m,p,Î·))
    uâº= xâº * sqrt(g(xâº,p,Î·))

    iteration = 1
    while true
        u = rand(Uniform(0,1))*uâº
        v = rand(Uniform(0,1))*vâº
        x = u/v
        if v^2 <= g(x,p,Î·) || iteration == max_iteration
            iteration == max_iteration && verbose && @warn "GIG sampling reached max iteration: check results carefully"
            return x
        end
        iteration += 1
    end
end

#Sample the 2-parameter GIG distribution with parameters p and Î· using the
#Ratio-of-Uniforms with mode shift as described by HÃ¶rmann & Leydold (2014) (originally from Dagpunar (1989)).
function sample_unif_mode_shift(p::Real, Î·::Real; max_iteration::Integer = 10000, verbose::Bool = false)
    m = (sqrt((p - 1)^2 + Î·^2) + (p-1)) / Î·
    a = -(2*(p+1)/Î·) - m
    b = (2*(p-1)/Î·) * m - 1
    p2 = b - (a^2)/3
    q = (2*a^3)/27 - (a*b/3) + m
    Ï• = acos(-(q/2) * sqrt(-27 / (p2^3)))
    xâ» = sqrt((-4/3)*p2)*cos(Ï•/3 + (4/3)*Ï€) - (a / 3)
    xâº = sqrt((-4/3)*p2)*cos(Ï•/3) - (a/3)
    vâº = sqrt(g(m,p,Î·))
    uâ» = (xâ» - m)*sqrt(g(xâ»,p,Î·))
    uâº = (xâº - m)*sqrt(g(xâº,p,Î·))

    iteration = 1
    while true
        u = rand(Uniform(0,1))*(uâº - uâ») + uâ»
        v = rand(Uniform(0,1))*vâº
        x = (u / v) + m
        if (x > 0 && v^2 <= g(x,p,Î·)) || iteration == max_iteration
            iteration == max_iteration && verbose && @warn "GIG sampling reached max iteration: check results carefully"
            return x
        end
        iteration += 1
    end
end

function g(x::Real,p::Real,Î·::Real)
    x^(p-1)*exp(-(Î·/2)*(x + (1/x)))
end